# IF1014-Mining

Projeto de MineraÃ§Ã£o de Dados desenvolvido para a disciplina IF1014, ministrada pelo professor Leandro Maciel Almeida sobre o nome TÃ³picos AvanÃ§ados em SI4 - Data Mining. Este projeto utiliza o dataset de geolocalizaÃ§Ã£o Nomao sob diversos algoritmos de Machine Learning.

## ğŸ“Š Sobre o Dataset

O dataset Nomao Ã© um conjunto de dados de classificaÃ§Ã£o binÃ¡ria que contÃ©m 120 features baseadas em geolocalizaÃ§Ã£o, seu intuito Ã© o desafio da deduplicaÃ§Ã£o. Nomao era conhecido como um motor de busca de lugares, agregando informaÃ§Ãµes sobre estabelecimentos e pontos de interesse. Sua base de dados era formada a partir de mÃºltiplas fontes (web, GPS, usuÃ¡rios etc.), resultando em um volume expressivo de registros.


## Nomao Challenge
 
CompetiÃ§Ã£o de Data Mining realizada em 2012, com foco na deduplicaÃ§Ã£o de registros de locais. O desafio central era criar modelos capazes de identificar se duas entradas correspondiam ao mesmo lugar, ou seja, consistia nos desafios da deduplicaÃ§Ã£o.


## Qual o porquÃª de deduplicaÃ§Ã£o?

- Qualidade dos Dados: A existÃªncia de registros duplicados prejudica buscas, relatÃ³rios e anÃ¡lises, afetando a confiabilidade do sistema.

- DecisÃµes EstratÃ©gicas: Empresas que dependem de dados geoespaciais (logÃ­stica, mapas digitais, delivery etc.) precisam de dados limpos e precisos para tomar decisÃµes acertadas.

## Metas

- DeduplicaÃ§Ã£o Eficiente: Desenvolver um modelo capaz de identificar se dois registros se referem ao mesmo local, reduzindo drasticamente o nÃºmero de duplicatas.

- Melhoria de Indicadores: AlcanÃ§ar mÃ©tricas robustas (por exemplo, F1-score  elevado) que atestem a eficÃ¡cia do modelo, em especial comparando com a literatura que encontramos.




## ğŸ—‚ï¸ Estrutura do Projeto

```
IF1014-Mining/
â”œâ”€â”€ README.md
â”œâ”€â”€ Data/
â”‚   â”œâ”€â”€ Nomao.features      # DescriÃ§Ã£o das features
â”‚   â”œâ”€â”€ Nomao.names         # Metadados do dataset
â”‚   â”œâ”€â”€ test.csv            # Conjunto de teste
â”‚   â””â”€â”€ train.csv           # Conjunto de treino
â””â”€â”€ Notebooks/
    â”œâ”€â”€ exploratory.ipynb   # AnÃ¡lise exploratÃ³ria dos dados
    â”œâ”€â”€ dt.ipynb            # Decision Tree
    â”œâ”€â”€ knn.ipynb           # K-Nearest Neighbors
    â”œâ”€â”€ random_forest.ipynb # Random Forest
    â”œâ”€â”€ mlp.ipynb           # Multi-Layer Perceptron
    â”œâ”€â”€ mlp_ensemble.ipynb  # Ensemble de MLPs
    â”œâ”€â”€ stacking.ipynb      # Stacking Classifier
    â””â”€â”€ z.ipynb             # Notebook auxiliar
```

## ğŸ¤– Modelos Implementados

### 1. Decision Tree (dt.ipynb)
- Classificador baseado em Ã¡rvore de decisÃ£o
- HiperparÃ¢metros otimizados: criterion, max_depth, min_samples_split, min_samples_leaf
- **Melhor F1-Score**: ~0.9415

### 2. K-Nearest Neighbors (knn.ipynb)
- Algoritmo baseado em distÃ¢ncia entre vizinhos
- HiperparÃ¢metros otimizados: n_neighbors, weights, p (distÃ¢ncia)
- **Melhor F1-Score**: ~0.9461

### 3. Random Forest (random_forest.ipynb)
- Ensemble de Ã¡rvores de decisÃ£o
- HiperparÃ¢metros otimizados: n_estimators, max_depth, min_samples_split, min_samples_leaf, bootstrap
- **Melhor F1-Score**: ~0.9610

### 4. Multi-Layer Perceptron (mlp.ipynb)
- Rede neural feedforward
- HiperparÃ¢metros otimizados: hidden_layer_sizes, activation, solver, alpha, learning_rate_init
- **Melhor F1-Score**: ~0.9517

### 5. MLP Ensemble (mlp_ensemble.ipynb)
- ComitÃª de redes neurais MLP
- Combina mÃºltiplas MLPs usando voting

### 6. Stacking Classifier (stacking.ipynb)
- **Melhor modelo do projeto**
- Combina KNN, Random Forest e LightGBM
- Meta-modelo: Logistic Regression
- **Melhor F1-Score**: ~0.9644
- **AcurÃ¡cia**: 0.9708
- **AUC**: 0.9951

## ğŸ› ï¸ Metodologia

### PrÃ©-processamento
- Balanceamento de classes com **SMOTE** (Synthetic Minority Over-sampling Technique)
- Pipeline de processamento usando `imblearn`

### OtimizaÃ§Ã£o de HiperparÃ¢metros
- **RandomizedSearchCV** com validaÃ§Ã£o cruzada estratificada (5 folds)
- MÃºltiplas rodadas de otimizaÃ§Ã£o (20 iteraÃ§Ãµes) para cada modelo
- MÃ©trica principal: **F1-Score Macro**

### ValidaÃ§Ã£o
- ValidaÃ§Ã£o cruzada estratificada (StratifiedKFold)
- Conjunto de teste separado para avaliaÃ§Ã£o final
- MÃ©tricas avaliadas: AcurÃ¡cia, PrecisÃ£o, Recall, F1-Score, AUC-ROC

## ğŸ“ˆ Resultados

| Modelo | F1-Score | AcurÃ¡cia | AUC |
|--------|----------|----------|-----|
| Decision Tree | 0.9415 | - | - |
| KNN | 0.9461 | 0.9600 | - |
| Random Forest | 0.9610 | - | - |
| MLP | 0.9517 | - | - |
| **Stacking** | **0.9644** | **0.9708** | **0.9951** |

## ğŸš€ Como Executar

### Requisitos
```bash
pip install numpy pandas matplotlib scikit-learn imbalanced-learn lightgbm jupyter
```

### Executar Notebooks
```bash
jupyter notebook
```

Navegue atÃ© a pasta `Notebooks/` e abra o notebook desejado.

## ğŸ“ Notebooks

- **exploratory.ipynb**: AnÃ¡lise exploratÃ³ria inicial dos dados
- **dt.ipynb**: ImplementaÃ§Ã£o e otimizaÃ§Ã£o do Decision Tree
- **knn.ipynb**: ImplementaÃ§Ã£o e otimizaÃ§Ã£o do KNN
- **random_forest.ipynb**: ImplementaÃ§Ã£o e otimizaÃ§Ã£o do Random Forest
- **mlp.ipynb**: ImplementaÃ§Ã£o e otimizaÃ§Ã£o do MLP
- **mlp_ensemble.ipynb**: Ensemble de MLPs
- **stacking.ipynb**: Modelo de Stacking (melhor resultado)

## ğŸ” Destaques TÃ©cnicos

- Uso de **Pipelines** para garantir reprodutibilidade
- **SMOTE** para lidar com desbalanceamento de classes
- **RandomizedSearchCV** para otimizaÃ§Ã£o eficiente de hiperparÃ¢metros
- **ValidaÃ§Ã£o cruzada estratificada** para avaliaÃ§Ã£o robusta
- **Stacking** com meta-modelo para combinar mÃºltiplos algoritmos

## ğŸ“Š VisualizaÃ§Ãµes

Todos os notebooks incluem:
- Matriz de confusÃ£o
- Curva ROC
- GrÃ¡ficos de evoluÃ§Ã£o do F1-Score ao longo das iteraÃ§Ãµes
- RelatÃ³rio de classificaÃ§Ã£o detalhado

## ğŸ‘¥ Autor

Desenvolvido para a disciplina IF1014 - MineraÃ§Ã£o de Dados

## ğŸ“„ LicenÃ§a

Este projeto Ã© de cÃ³digo aberto para fins educacionais.

---

**Nota**: O modelo de Stacking apresentou o melhor desempenho geral, combinando as forÃ§as de diferentes algoritmos (KNN, Random Forest e LightGBM) com um meta-modelo de RegressÃ£o LogÃ­stica.
